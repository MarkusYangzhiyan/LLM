{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f2cc3c",
   "metadata": {},
   "source": [
    "### 首先我们回顾一下Chapter1的内容\n",
    "### 如何使用transformer下载部署一个大模型\n",
    "### 如何加载这个大模型，如何在交互前预设"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc5dff7",
   "metadata": {},
   "source": [
    "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9bc007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4207d3b6",
   "metadata": {},
   "source": [
    "### 1. 使用model 和 tokenizer 的基础方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1298d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "HF_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"cuda\",                           # 使用gpu\n",
    "    torch_dtype = \"auto\",\n",
    "    trust_remote_code = True,                       # 调用外部模型（非transformers内预设的）\n",
    "    #token = HF_TOKEN                               # 第一次下载模型时使用\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8aec53c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b1e1e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>',\n",
       "  '<|im_end|>',\n",
       "  '<|object_ref_start|>',\n",
       "  '<|object_ref_end|>',\n",
       "  '<|box_start|>',\n",
       "  '<|box_end|>',\n",
       "  '<|quad_start|>',\n",
       "  '<|quad_end|>',\n",
       "  '<|vision_start|>',\n",
       "  '<|vision_end|>',\n",
       "  '<|vision_pad|>',\n",
       "  '<|image_pad|>',\n",
       "  '<|video_pad|>']}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)#,token = HF_TOKEN)\n",
    "\n",
    "\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bce3fd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是一个历史学家，精通中国历史上下五千年<|im_end|>\n",
      "<|im_start|>user\n",
      "请问诗仙是谁？简单介绍一下他，并且给出一首他的诗<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"你是一个历史学家，精通中国历史上下五千年\"\n",
    "user_prompt = \"请问诗仙是谁？简单介绍一下他，并且给出一首他的诗\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":system_prompt},\n",
    "    {\"role\":\"user\", \"content\":user_prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True\n",
    ")\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e3676870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,  56568, 101909, 100022, 108026,   3837, 114806,\n",
      "          58695, 100022, 102285,  75108, 106705, 151645,    198, 151644,    872,\n",
      "            198, 109194, 100045, 100717, 105518,  11319, 100405, 109432,  42411,\n",
      "          90395, 100136, 107485, 108462, 100648, 100045, 151645,    198, 151644,\n",
      "          77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer([text],return_tensors = \"pt\").to(model.device)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "467a8566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以，以下是一首出自唐朝诗人李白的《静夜思》：\n",
      "\n",
      "床前明月光，疑是地上霜。\n",
      "举头望明月，低头思故乡。\n"
     ]
    }
   ],
   "source": [
    "generaeted_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens = 512,    \n",
    ")\n",
    "\n",
    "generaeted_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids,output_ids in zip(model_inputs.input_ids,generaeted_ids)\n",
    "\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids,skip_special_tokens = True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f19cb9",
   "metadata": {},
   "source": [
    "### 2. 使用transformers的pipeline来简化流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c77f2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype = \"auto\",\n",
    "    device_map = \"cuda\",\n",
    "    trust_remote_code = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "82c369b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "51071a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "988db555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    max_new_tokens = 512,\n",
    "    model  = model,\n",
    "    tokenizer = tokenizer,\n",
    "    return_full_text = False,\n",
    "    do_sample = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "99521aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李白（701年2月8日－762年），字太白，号青莲居士，唐代伟大的浪漫主义诗人，被后人称为“诗仙”。其作品以七言绝句和三首七言律诗最著名。代表作有《将进酒》、《庐山谣》等。\n"
     ]
    }
   ],
   "source": [
    "output = generator(messages)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff192cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huoshan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

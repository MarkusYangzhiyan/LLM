import numpy as np
import time
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import fashion_mnist

# å¯¼å…¥ä½ çš„æ¨¡å—
from linear import Linear
from activations import ReLU
from loss import CrossEntropyloss
from optimizer import Adam, SGD
from dropout import Dropout
from batchnorm import BatchNorm
from utils import calculate_accuracy
from dataloader import DataLoader

# ç®€æ˜“æ¨¡å‹å®¹å™¨
class Sequential:
    def __init__(self, layers):
        self.layers = layers
    def forward(self, x):
        for layer in self.layers: x = layer.forward(x)
        return x
    def backward(self, grad):
        for layer in reversed(self.layers): grad = layer.backward(grad)
    def train(self):
        for layer in self.layers: layer.train()
    def eval(self):
        for layer in self.layers: layer.eval()
    def get_params(self):
        return [p for layer in self.layers if hasattr(layer, 'params') for v in layer.params.values() for p in [v]]
    def get_grads(self):
        return [g for layer in self.layers if hasattr(layer, 'params') for v in layer.grads.values() for g in [v]]

# å®šä¹‰ç½‘ç»œ
def build_fashion_net():
    # ç»“æ„: 784 -> 256 -> 128 -> 10
    layers = [
        Linear(784, 256), BatchNorm(256), ReLU(), Dropout(0.2),
        Linear(256, 128), BatchNorm(128), ReLU(),
        Linear(128, 10)
    ]
    return Sequential(layers)

# å®éªŒé€»è¾‘
def run_experiment(name, opt_class, lr, x_train, y_train, x_test, y_test):
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {name} (lr={lr})")
    np.random.seed(42)
    model = build_fashion_net()
    criterion = CrossEntropyloss()
    
    # åŒºåˆ† SGD éœ€è¦ momentum å‚æ•°
    if name == "SGD":
        optimizer = opt_class(model.get_params(), lr=lr, momentum=0.9)
    else:
        optimizer = opt_class(model.get_params(), lr=lr)

    train_loader = DataLoader(x_train, y_train, batch_size=128, shuffle=True)
    test_loader = DataLoader(x_test, y_test, batch_size=128, shuffle=False)
    
    loss_history = []
    acc_history = []
    
    for epoch in range(5): # è·‘ 5 ä¸ª Epoch
        model.train()
        epoch_loss = 0
        for x_batch, y_batch_onehot in train_loader:
            logits = model.forward(x_batch)
            loss = criterion.forward(logits, y_batch_onehot)
            model.backward(criterion.backward())
            optimizer.step(model.get_grads())
            epoch_loss += loss
            
        # éªŒè¯
        model.eval()
        correct = 0
        total = 0
        for x_b, y_b_onehot in test_loader:
            out = model.forward(x_b)
            correct += calculate_accuracy(out, y_b_onehot) * x_b.shape[0]
            total += x_b.shape[0]
            
        avg_loss = epoch_loss / len(train_loader)
        acc = correct / total
        loss_history.append(avg_loss)
        acc_history.append(acc)
        print(f"   Epoch {epoch+1} | Loss: {avg_loss:.4f} | Test Acc: {acc:.4f}")
        
    return loss_history, acc_history

if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
    # é¢„å¤„ç†
    x_train = x_train.reshape(-1, 784).astype(np.float32) / 255.0
    x_test = x_test.reshape(-1, 784).astype(np.float32) / 255.0

    # å¯¹æ¯”è®­ç»ƒ
    loss_sgd, acc_sgd = run_experiment("SGD", SGD, 0.01, x_train, y_train, x_test, y_test)
    loss_adam, acc_adam = run_experiment("Adam", Adam, 0.001, x_train, y_train, x_test, y_test)

    # ç”»å›¾
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(loss_sgd, label='SGD')
    plt.plot(loss_adam, label='Adam')
    plt.title('Fashion-MNIST Loss')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(acc_sgd, label='SGD')
    plt.plot(acc_adam, label='Adam')
    plt.title('Fashion-MNIST Accuracy')
    plt.legend()
    plt.show()